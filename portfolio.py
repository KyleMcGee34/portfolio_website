import streamlit as st
from streamlit_option_menu import option_menu
from PIL import Image
import pandas as pd

syr_img = Image.open('images/Syracuse_Orange_logo.png')
st.set_page_config(layout="wide", page_title="Kyle McGee's Portfolio", page_icon=syr_img)


'''# Kyle McGee's Portfolio'''


with st.sidebar:
    choose = option_menu("Navigation Menu", ["Overview", "Project Descriptions", "Recorded Video Presentation", "Blog Post", "Contact"],
                         menu_icon="app-indicator", default_index=0,
                         styles={
        "container": {"padding": "5!important", "background-color": "#fafafa"},
        "icon": {"color": "orange", "font-size": "25px"}, 
        "nav-link": {"font-size": "14px", "text-align": "left", "margin":"0px", "--hover-color": "#eee"},
        "nav-link-selected": {"background-color": "#02ab21"},
    }
    )

if choose == "Overview":
    '''## Overview'''
    '''The M.S. in Applied Data Science program aims to equip students with the skills and knowledge necessary to succeed as data scientists in various professional fields. The program was designed around six key learning objectives, which include the ability:'''
    '''1.	To collect, store, and access data by identifying and leveraging applicable technologies'''
    '''2.	To create actionable insights across a range of contexts using the full data science life cycle'''
    '''3.	To apply visualization and predictive models to help generate actionable insights'''
    '''4.	To use programming languages such as R and Python to support the generation of actionable insights'''
    '''5.	To communicate insights gained via visualization and analytics to a broad range of audiences'''
    '''6.	To apply ethics in the development, use, and evaluation of data and predictive models.'''
    '''The following projects I completed during my studies demonstrated the learning outcomes of the M.S. in Applied Data Science program. Each project had unique goals, utilized various technologies, and generated actionable insights.'''
    '''**Predicting the NFL**: This was a group project where the goal was to solve a real-world business problem using machine learning techniques. We decided to analyze the American game of football through the NFL. We aimed to provide actionable insight for NFL teams and coaches by predicting if any given play would be a run or a pass.'''
    '''**Worldwide power plant data analysis**: This was a group project where the primary objective was to write Python scripts to access and accumulate data. As a group, we decided to analyze powerplant data from around the world. Given the growing concern about the world's climate, we aimed to provide a comprehensive power plant analysis to our audience.'''
    '''**Hotel cancellations**: This was a project I took upon independently. The main objective was to identify significant factors contributing to hotel reservation cancellations. Through a combination of data analysis and predictive models, I hoped to help a hotel manager identify potential cancellations.'''
    '''Pooled together, the three projects mentioned touched upon the learning objectives and helped me in my journey to become a data scientist. Each project utilized either R, Python, or a combination of both to accomplish its end goal, which was either in-depth analysis or actionable insight. All three projects had different key stakeholders, making each one unique. In addition, visualizations and/or predictive models were used to explain and reinforce the findings. At the end of each project, I presented the findings to my peers, who had varying levels of domain knowledge on the subject. For instance, for the hotel cancellations project, since everyone used the same dataset, I could present technical details. Conversely, the power plant analysis project had to be presented at a higher level since nobody had seen the dataset. For group projects, we found it most practical to store data on cloud storage rather than locally. This ensured that every group member had access to the same dataset. In this case, we didn't require databases but opted for services like Google Drive and GitHub. Lastly, the original dataset for predicting the NFL project contained biased information about how successful the current play was. As a result, ethics played a crucial role in determining how to handle this information.'''
    
if choose == "Project Descriptions":
    '''## Project Descriptions'''
    
    tab1, tab2, tab3 = st.tabs(["Predicting the NFL", "Worldwide Power Plant Data Analysis", "Hotel Cancellations"])
    
    with tab1:
        
        '''### Predicting the NFL'''
        
        '''#### Introduction'''

        '''The ability to anticipate play-calling tendencies during National Football League games provides a powerful means to promote game-winning performance. Using 307,317 observations of pre-snap, play-by-play data from the 2011 to 2020 NFL seasons, this project employs a series of machine learning techniques, including decision trees, random forest, and gradient boosting, to predict if any given play in the 2020 NFL season was a run or a pass. The main objective of this project was to determine the predictability of specific teams. This project provides a variety of value-adds to NFL coaches, offensive coordinators, talent scouts, and players.'''

        '''#### Subsetting of Features'''

        '''The first step in this project involved extensive data cleaning and feature engineering techniques. We acquired our dataset from nflfastR, a library in R. After pulling the play-by-play data from the 2011-2022 seasons, we had 372 different variables. In this project, we chose to focus on pre-snap features to avoid making predictions based on the results of the play. Features such as how many yards were gained, where the ball was run, what player made the tackle, etc. were removed. Additional features removed included certain external observations such as vegas betting odds, meta identifiers, etc. After this subset was selected, observations that were not a run or pass were removed from the dataset. This included kickoffs, field goals, two-point conversions, punts, extra points, quarterback spikes, quarterback kneels, no plays, and missing values.'''
        
        original_features = pd.DataFrame(
            [('play_type','Type of play that the observation is (Run, Pass, Kickoff, etc.)'),('play_id','ID (unique identifier when paired with game_id)'), ('game_id','Ten digit ID (unique identifier when paired with play_id)'), ('score_differential','Score differential between the team on offense and defense at the start of the play'), ('home_team','Home team'), ('away_team','Away team'), ('season_type','Regular season or post-season'), ('week','Week the game took place in'), ('posteam','Team that is on offense'), ('defteam','Team that is on defense'), ('posteam_type','Home or away'), ('side_of_field','Side of the field the offense is currently on'), ('yardline_100',"Distance in yards from the opponent's end zone for the team that is on offense"), ('half_seconds_remaining', 'Seconds remaining in the half'), ('game_seconds_remaining', 'Seconds remaining in the game'), ('game_half', 'Half1, half2, or overtime'), ('drive', 'Drive number for the offensive team'), ('qtr', 'Quarter of the game'), ('down', 'Down for the given play'), ('goal_to_go', 'Indicates if the offense is in a goal down situation'), ('ydstogo', 'Distance in yards to first down marker or end zone in goal down situations'), ('desc', 'Detailed description of given play'), ('no_huddle', 'Indicates if the play offensive team did not huddle up before the play'), ('shotgun', 'Indicated if the QB is in shotgun formation'), ('posteam_timeouts_remaining','Number of timeouts that the offense has left'), ('wp','Win probability for the team on offense at the start of the play'), ('no_score_prob','Probability of no point being score by the team on offense for the rest of the half'), ('fg_prob','Probability of the team on offense scoring a field goal next play'), ('saftey_prob','Probability of the team on offense scoring a safety next'), ('td_prob', 'Probability of the team on offense scoring a touchdown next play'), ('roof','Indicates if there is a roof on the stadium'), ('surface', 'Indicates the type of surface that the game is being played on'), ('temp','Temperature at the start of the game'), ('wind', 'Wind speed at the start of the game'), ('game_stadium', 'Name of the stadium the game was played in'), ('passer','Quarterback in the game'), ('rusher', 'Running back in the game'), ('receiver', 'Receiver in the game'), ('home_coach', 'Coach of the home team'), ('away_coach', 'Coach of the away team'), ('epa','Expected points added by the team on offense for the given play'), ('total_home_rush_epa', 'Cumulative rushing EPA for the home team in the game so far'), ('total_away_rush_epa', 'Cumulative rushing EPA for the away team in the game so far'), ('total_home_pass_epa', 'Cumulative passing EPA for the home team in the game so far'), ('total_away_pass_epa', 'Cumulative passing EPA for the away team in the game so far')],
            columns=['Feature Name','Feature Description'])
        
        with st.expander("Show original features that were kept"):
            st.table(original_features)
            
        '''#### Feature Engineering'''
        
        '''To reduce the number of categorical features a new feature was created - “pos_coach” - which designated the head coach of the team on offense for each observation. When the home team was the same as the team on offense, the pos_coach column was filled in with the value in the home_coach column and vice-versa for the away team. After this process was complete, the home_coach and away_coach columns were dropped.'''
        
        '''The original dataset contained features that specified who the quarterback/passer (QB), running back/rusher (RB), and wide receiver/receiver (WR) were for each play. Most of these columns contained missing values because they were only populated when the corresponding player touched the ball for each observation. For example, if Tom Brady threw a pass to Rob Gronkowski, Brady would be filled in as the QB and Gronkowski as the WR. The rusher column would be NA in this case. If Marshawn Lynch ran the ball, Lynch would be filled in as the RB and both the passer and receiver columns would be NA. The players on the field who physically touch the ball and are involved in moving it downfield at the start of a play (i.e. QB, RB, WR) could indicate if a play was going to be a pass or a run. As such, the NA values in the passer, rusher, and receiver columns were populated using the following process: For a given team in a given game, a value counts table was created. The player that appeared most in a given game for a given team replaced the NA values in the appropriate column. This served as a best guess for whom the passer, rusher, and receiver were on any given play.'''
        
        '''Expected points added (EPA) serve as a numeric indicator for how successful/unsuccessful any given play was. On every play, the offensive team can either increase or decrease their expected points. A positive EPA would roughly indicate that the play was a success, e.g. yards were gained, and vice-versa for a negative EPA. EPA data was kept for every observation, as well as a cumulative EPA for each run and each pass observation in a given game because these features provided useful information about how successful the previous play was without detailing the exact results of said play. Moreover, cumulative EPA provided useful information about how successful a given team had been as of each observation running and/or passing the ball in a given game. The EPA data from the original nflfastR dataset was directly related to the play that just occurred, i.e., the observation that our models were making a prediction for. Due to this, the EPA observations needed to be staggered so that the next play for a given team in a given game had all of the EPA values associated with the previous play(s). This was imperative to do, especially with the cumulative EPA data, in order to avoid models noticing that the cumulative run EPA increased or decreased whenever the cumulative pass EPA stayed the same. Without staggering the data, predictive models potentially could have inferred that a team ran the ball under these circumstances and vice-versa for passing the ball when the cumulative run EPA stayed the same between observations.'''
        
        '''To monitor the ratio of runs to passes for a given team, additional features were created. Especially late in a game, this ratio could be useful for determining if a subsequent play is more likely to be a run or a pass. First, a count of runs and passes was calculated for each team in each given game. These counts were staggered in a similar way as the EPA data described earlier. The first observation in each game for a given team was 0 for both the count of runs and the count of passes. These fields were updated in the subsequent observation for a given team in a given game depending on if the corresponding team ran or passed the ball in the previous observation. This was done to ensure that the result of the play was not known. After the count of runs and the count of passes were staggered, an in-game run-to-pass ratio was calculated. To further characterize a team's running and passing tendencies, additional fields were calculated to track the run-to-pass ratio for a given down (first, second, third, and fourth). The same staggering logic was applied to these grouped calculations.'''
        
        '''The quarterback (QB) is the player in charge of the offense as they are the first one to touch the ball on most, if not all, possessions. They are also often responsible for calling the play in the huddle, changing the play at the line of scrimmage, handing the ball off, passing the ball, etc. Due to all these responsibilities, how good the QB is for a certain team could dictate what the offense does in certain situations. In order to account for the importance of the QB, the well-known passer rating, also known as QBR (quarterback rating), was used to track QB performance per game. Before calculating this feature, week-by-week player statistics were obtained using the load_player_stats module within nflfastR. Once this data was obtained, each QBs passer rating was calculated using the following five variables: pass attempts, completions, passing yards, touchdown passes, and interceptions.'''
        
        with st.expander("Show QBR calculation"):
            st.image('images/QBR.png')
            
        '''Analogous to the skill of a QB being indicative of a play being a run or a pass, how effective a given team’s defense is could also influence play-calling in certain situations. In order to obtain a measure of how good a team's defense is, the data was sublimated with statistics from Pro Football Reference. The rankings of a given team’s defense in a given season are based on their performance in the previous season. The rankings range from whole-number values of 1 (the team with the least points allowed during the previous season) to 32 (the team with the most points allowed during the previous season).'''
        
        feature_engineering = pd.DataFrame(
        [('pos_coach','Coach of the team that is on offense'), ('epa', 'EPA based on the previous play'), ('total_rush_epa', 'Cumulative run EPA (in-game)'), ('total_pass_epa', 'Cumulative pass EPA (in-game)'), ('season_rush_epa', 'Cumulative run EPA (season)'), ('season_pass_epa', 'Cumulative pass EPA (season)'), ('number_of_runs', 'Run plays (in-game)'), ('number_of_pass', 'Pass plays (in-game)'), ('number_of_runs_season', 'Run plays (season)'), ('number_of_pass_season', 'Pass plays (season)'), ('season_pass_run_ratio', 'Pass:run ratio (season)'), ('in_game_pass_run_ratio', 'Pass:run ratio (in-game)'), ('number_of_runs_first_down', 'Run plays on first down (in-game)'), ('number_of_pass_first_down', 'Pass plays on first down (in-game)'), ('number_of_runs_second_down', 'Run plays on second down (in-game)'), ('number_of_pass_second_down', 'Pass plays on second down (in-game)'), ('number_of_runs_third_down', 'Run plays on third down (in-game)'), ('number_of_pass_third_down', 'Pass plays on third down (in-game)'), ('number_of_runs_fourth_down', 'Run plays on fourth down (in-game)'), ('number_of_pass_fourth_down', 'Pass plays on fourth down (in-game)'), ('in_game_pass_ratio_per_down', 'Pass:run ratio per down (in-game)'), ('QBR', "The passer's calculated quarterback rating"), ('defense_ranking', 'Rank of the team that is on defense (1=best, 32=worst)')],
        columns=['Feature Name','Feature Description'])
    
        with st.expander("Show feature engineering columns"):
            st.table(feature_engineering)
            
        '''#### Model Construction'''
        
        '''Three different types of machine learning models were explored to make predictions in this project: decision tree, random forest, and gradient boosting. For each iteration of these model types, the data was split into training and testing sets. The training data was manually composed of all observations from the 2011 to 2019 season. As such, the test data that these models were evaluated against contains all the run and pass plays from the 2020 NFL season.'''
        
        '''#### Results & Discussion'''
        
        '''Since there are 32 different NFL teams, 32 different models were created. The three different model types that were explored came from the sklearn package in python. Each team, along with their corresponding play-by-play data for the 2011 to 2019 seasons, underwent a rigorous parameter search. The best models were classified as those which obtained the highest test accuracy when making predictions for the 2020 season.'''
        
        col1, col2 = st.columns(2)
        
        with col1:
            '''Utilizing the best models for each team, we were able to break down the predictability of a team based on different in-game scenarios. Off to the right is a breakdown, by down, of how accurate our team-based models were at predicting a run or pass for each team. One down that would be of particular interest to NFL coaches and teams is first down. First down is the most unpredictable down in the game of football. If the team on defense has a good idea of the play being a run or pass on first down, their chances of stopping the offense increase. Preventing the offense from gaining yards on first down will often result in a situation where they need to gain even more yards on second down, therefore pressuring them to play more aggressively. In the models we have built, the Cleveland Browns are the most predictable team on first down with a test accuracy of around 75%. Alternatively, the Houston Texans, the team with the lowest test accuracy on first down, would be pleased to know that our models have a tough time predicting what they are going to do on first down.'''
        with col2:
            with st.expander("Best Accuracies for Each Team per Down", expanded=True):
                st.image('images/Down.png')
                
        col1_1, col2_1 = st.columns(2)
        
        with col1_1:
            '''The same approach was used to represent team accuracy per quarter which can be seen in the figure to the right. When it comes to the most predictable teams per quarter, on average, teams are more predictable in the second and fourth quarters. Analogous to the analysis of teams per down, NFL coaches and teams might be interested in the predictability of another team in the first quarter. If a defense can accurately predict whether the offense will run or pass the ball, they have a higher chance of stopping them and hence will be in a better position. Frequently stopping a team in the first quarter will inevitably give the defense a higher chance of winning the game. Teams playing against the Detroit Lions would be pleased to know that with our models, they can predict to an accuracy of 75% whether the Lions are going to run or pass the ball in the first quarter. Alternatively, the Philadelphia Eagles would be pleased to know that they are the least predictable team in the first and third quarters.'''
        
        with col2_1:
            with st.expander("Best Accuracies for Each Team per Quarter", expanded=True):
                st.image('images/Quarter.png')
        
        col1_2, col2_2 = st.columns(2)
        
        with col1_2:
            '''A different kind of analysis was deployed in the figure to the right that NFL teams could be interested in. The heatmap shows how predictable an offense (y-axis) is when playing a particular defense (x-axis). In the NFL, there are eight divisions comprised of four teams, and a team plays other teams in their division twice. Zeros indicate either that the two teams didn’t face each other or that they are the same team. From the offense's perspective, a lower test accuracy when paired up with a certain defense signifies that our models perform worse when they play that certain team, meaning that their predictability is low. Alternatively, our models would be of interest to the defensive team when the test accuracy is high.'''
        
        with col2_2:
            with st.expander("Accuracy of a given Offense against a given Defense in 2020", expanded=True):
                st.image('images/OffvDef.png')
                
        '''There are a variety of future applications for this project. Game-by-game sequences - such as if the previous game was a win or a loss for a given team or the amount of time that has passed between games - could provide interesting and meaningful insights for NFL personnel. Related to this is the exploration of additional features, such as if players were injured or benched in between games or the overall turnover of a team’s personnel between seasons, that could improve the explainability or predictive capacity of these models.'''
    
    with tab2:
        '''### Worldwide Power Plant Data Analysis'''
        
        '''#### Introduction'''
        
        '''In today’s current climate, energy resources seem to be at the forefront of many discussions. Whether it’s about energy independence to reduce reliance on competing countries, or concerning climate change and the need to reduce reliance on fossil fuels, energy capacity and generation is an important topic for all to understand. This project aims to help individuals understand the global distribution of power plants in capacity, fuel type, and other key metrics to help foster a more in-depth conversation on the world’s energy picture.'''
        
        '''#### Data Preprocessing'''
        
        '''The data that was used in this project came from the World Resources Institute (WRI) which is a non-profit organization that works with businesses, governments, and other groups to develop solutions to improve people’s lives. The WRI built the dataset using open sources which can be found [here](https://www.wri.org/research/global-database-power-plants). The original dataset contains 34,936 rows and 36 columns. The first step in preparing this dataset for further analysis involved the subsetting of unimportant features. A list of the variables that we choose to get rid of and keep can be seen in the tables below.'''
        
        original_features_kept = pd.DataFrame(
            [('country', 'text', '3 character country code corresponding to the ISO 3166-1 alpha-3 specification'), ('country_long', 'text', 'longer form of the country designation'), ('name', 'text', 'name or title of the power plant, generally in Romanized form'), ('capacity_mw', 'number', 'electrical generating capacity in megawatts'), ('latitude', 'number', 'geolocation in decimal degrees; WGS84 (EPSG:4326)'), ('longitude', 'number', 'geolocation in decimal degrees; WGS84 (EPSG:4326)'), ('primary_fuel', 'text', 'energy source used in primary electricity generation or export'), ('commissioning_year', 'number', 'year of plant operation, weighted by unit-capacity when data is available'), ('owner', 'text', 'majority shareholder of the power plant, generally in Romanized form'), ('source', 'text', 'entity reporting the data; could be an organization, report, or document, generally in Romanized form'), ('url', 'text', 'web document corresponding to the “source” field'), ('year_of_capacity_data', 'number', 'year the capacity information was reported'), ('generation_gwh_2013', 'number', 'electricity generation in gigawatt-hours reported for the year 2013'), ('generation_gwh_2014', 'number', 'electricity generation in gigawatt-hours reported for the year 2014'), ('generation_gwh_2015', 'number', 'electricity generation in gigawatt-hours reported for the year 2015'), ('generation_gwh_2016', 'number', 'electricity generation in gigawatt-hours reported for the year 2016'), ('generation_gwh_2017', 'number', 'electricity generation in gigawatt-hours reported for the year 2017'), ('generation_gwh_2018', 'number', 'electricity generation in gigawatt-hours reported for the year 2018'), ('generation_gwh_2019', 'number', 'electricity generation in gigawatt-hours reported for the year 2019'), ('estimated_generation_gwh_2013', 'number', 'estimated electricity generation in gigawatt-hours for the year 2013'), ('estimated_generation_gwh_2014', 'number', 'estimated electricity generation in gigawatt-hours for the year 2014'), ('estimated_generation_gwh_2015', 'number', 'estimated electricity generation in gigawatt-hours for the year 2015'), ('estimated_generation_gwh_2016', 'number', 'estimated electricity generation in gigawatt-hours for the year 2016'), ('estimated_generation_gwh_2017', 'number', 'estimated electricity generation in gigawatt-hours for the year 2017')],
            columns=['Label', 'Type', 'Description'])
        
        original_features_not_kept = pd.DataFrame(
            [('gppd_idnr', 'text', '10 or 12 character identifier for the power plant'), ('other_fuel1', 'text', 'energy source used in electricity generation or export'), ('other_fuel2', 'text', 'energy source used in electricity generation or export'), ('other_fuel3', 'text', 'energy source used in electricity generation or export'), ('geolocation_source', 'text', 'attribution for geolocation information'), ('wepp_id', 'text', 'a reference to a unique plant identifier in the widely-used PLATTS-WEPP database'), ('year_of_capacity_data', 'number', 'year the capacity information was reported'), ('generation_data_source', 'text', 'attribution for the reported generation information'), ('estimated_generation_note _2013', 'text', 'label of the model/method used to estimate generation for the year 2013'), ('estimated_generation_note _2014', 'text', 'label of the model/method used to estimate generation for the year 2014'), ('estimated_generation_note _2015', 'text', 'label of the model/method used to estimate generation for the year 2015'), ('estimated_generation_note _2016', 'text', 'label of the model/method used to estimate generation for the year 2016'), ('estimated_generation_note _2017', 'text', 'label of the model/method used to estimate generation for the year 2017')],
            columns=['Label', 'Type', 'Description'])
        
        col1_3, col2_3 = st.columns(2)
        with col1_3:
            with st.expander("Show original features that were kept"):
                st.table(original_features_kept)
        with col2_3:
            with st.expander("Show original features that were not kept"):
                st.table(original_features_not_kept)
                
        '''After some initial data exploration, it was quickly discovered that some variables had a significant amount of null values. Most of these columns were the “generation_gwh_year” columns. Since the subsequent analysis was going to rely heavily upon these columns, steps needed to be taken to reduce the number of null values. One approach involved filling in the null generation values for a given year with the estimated generation data for that year. For instance, the raw “generation_gwh_2017” column had around 72% null values. After filling it with the “estimated_generation_gwh_2017” around 4% of missing values remained. Another way to mitigate the missing data was to create two columns, one column with the average of the observed generation values in GWh, and the other with the estimated generation values. This way of mitigation provided a holistic view of power generation over a longer period of time.'''
        
        '''Lastly, to help with classification, a new column was created based on the value of “primary_fuel”. The following fuel types were classified as such:'''
        '''* **Renewable Energies**: Hydro, Solar, Wind, Wave and Tidal, Geothermal, Waste, and Biomass'''
        '''* **Non-Renewable Energies**: Gas, Oil, Nuclear, Coal, Petcoke, Storage, Cogeneration, and Other'''
        
        '''#### Data Analysis'''
        
        '''To understand the distribution of power plants by different fuel types, the plot below was created. It's clear that solar power plants dominate this dataset in terms of physical location followed by hydro and wind plants. This is a promising visual, as it shows there are more power plants producing renewable energy than non-renewable energy.'''
        
        with st.expander("Show count of plants by fuel type"):
            st.image('images/plants.png')
            
        '''To get a different perspective on the data, it’s important to investigate the energy production capacity of each fuel type. The visualization below tells a different story than the one above. Three of the top four fuel types in terms of energy capacity are non-renewable energy sources. Although coal power plants only top out at 5 in terms of the number of plants, the capacity, and energy they can produce is significantly higher than other fuel types.'''

        with st.expander("Show capacity of plants by fuel type"):
            st.image('images/plant_capacity.png')
            
        '''One of the questions that could be explored through the data was the following: are there any countries that use renewable energy for over 50% of their total power generation capacity? Answering this question will provide insight into which countries are leaders in using renewable energies more than using non-renewable energies. The table below shows the top 5 countries that have the largest energy capacity, whether it be renewable or nonrenewable, and their corresponding renewable percentage. The data is presented in this way because those countries that have a higher energy capacity, as well as a higher renewable energy percentage, signify that even though they have high generation needs, they are still trying to produce renewable energy. The most shocking revelation here is Norway where 95% of their energy capacity comes from renewable energy sources.'''
        
        with st.expander("Show generation and capacity of renewable energy by country"):
            st.image('images/renew50.png', caption='0.0 = non-renewable energy capacity. 1.0 = renewable energy capacity. totalcap = total energy capacity')
            
        '''This leads to the next questions which were: which Countries are leaders in renewable energy? How much renewable energy are they producing compared to non-renewable energy? Instead of investigating which countries were producing more than 50% renewable energy out of all sources, an analysis was conducted to investigate which countries have the highest renewable energy capacity. Again, the renewable energy percentage was calculated to give insight into the percentage of energy coming from renewable and non-renewable sources. The numbers are shown in the table below. China looks as if they have the capability of producing a lot of renewable energy, but when compared to its renewable sources, the perspective changes. The country that sticks out in this investigation is Brazil. They are the third highest country in renewable energy capacity which accounts for 83% of their total energy capacity.'''
        
        with st.expander("Show leaders in renewable energy capacity"):
            st.image('images/renewleaders.png', caption='0.0 = non-renewable energy capacity. 1.0 = renewable energy capacity. totalcap = total energy capacity')
            
        '''Energy capacity is one thing, but what about actual energy production? The next questions that were addressed are the following: which countries have the highest power generation capability? How does it relate to actual energy production? This analysis will provide insight into capacity and actual energy production. Just because a country has the capability of producing x amount of energy doesn’t necessarily mean they do. The plot below shows the top 20 countries in gW energy capacity and their corresponding generation of renewable and non-renewable energy in the year 2017. This plot shows a few things. First, the countries with higher populations have higher energy capacity and generation, which is obvious. Second, out of the 20 countries shown, only 2, Brazil and Canada, produce more renewable energy than non-renewable energy which was shown in the table above.'''
        
        with st.expander("Show top 20 countries in energy capacity and energy production"):
            st.image('images/top20.png')
            
        '''To gain a better understanding of today’s climate, analyzing the generation and consumption countries could provide valuable insight. After all, energy can be traded just like any other product. So just because one country is producing a lot of renewable energy doesn’t mean they are consuming the same amount. To begin this analysis, a dataset on consumption was taken from Our World Data. It can be found [here](https://ourworldindata.org/sources-global-energy). The main goal is to investigate any differences between consumption and generation for a particular country, in this case, the United States. Below are two pie charts that show the consumption and generation of energy in the US in the year 2017. It shouldn't be a shock to see that the US generates more non-renewable energy than renewable energy since this was shown earlier. When comparing the percentages of energy sources that the US uses, it is interesting to see that all non-renewable percentages increase while all renewable percentages decrease. Also, the consumption of oil is striking, making up more than 40% of all the energy that the US consumes. The key takeaway here is that energy generation data can be misleading. Comparing it with a country's energy consumption will provide more of a holistic viewpoint on the energy sector in that given country.'''
        
        with st.expander("Show US consumption and generation pie charts"):
            st.image('images/convgen.png')
            
        '''#### Summary'''
        
        '''In conclusion, the analysis of global energy production and consumption has revealed some interesting trends. The distribution of power plants by fuel type shows that there are more renewable energy plants than non-renewable energy plants. However, when looking at energy production capacity, non-renewable energy sources dominate the top spots. Norway stands out as a leader in using renewable energy sources for 95% of its energy capacity. Brazil also shows promise as they are the third-highest country in renewable energy capacity, accounting for 83% of its total energy capacity. But when looking at actual energy production, only two countries out of the top 20 produce more renewable energy than non-renewable energy. It is important to note that energy generation data can be misleading and comparing it with a country's energy consumption will provide a more comprehensive view of the energy sector in that given country.'''
                     
    with tab3:
        '''### Hotel Cancellations'''
        
        '''#### Introduction'''
        
        '''If a hotel manager could accurately predict whether their guests would cancel their reservation or not, they could increase profit by potentially overbooking rooms. This project aimed to provide key stakeholders with an in-depth analysis of the type of people who cancel hotel reservations and actionable insight into predicting who would cancel their reservation.'''
        
        '''#### Data Exploration'''
        
        '''This project began with data exploration. The original dataset contained 40,060 observations and 20 variables. Of the 20 variables, 7 were categorical, with the rest being numeric. Below is a table depicting the variables available in this project.'''
        
        hotel_original_features = pd.DataFrame(
            [('IsCanceled', 'Categorical', 'Value indicating if the booking was canceled (1) or not (0)'), ('LeadTime', 'Integer', 'Number of days that elapsed between the entering date of the booking into and the arrival date'), ('StaysInWeekendNights', 'Integer', 'Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel'), ('StaysInWeekNights', 'Integer', 'Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel'), ('Adults', 'Integer', 'Number of adults'), ('Children', 'Integer', 'Number of children'), ('Babies', 'Integer', 'Number of babies'), ('Meal', 'Categorical', 'Type of meal booked. Categories are presented in standard hospitality meal packages: Undefined/SC – no meal package; BB – Bed & Breakfast; HB – Half board (breakfast and one other meal – usually dinner); FB – Full board (breakfast, lunch and dinner)'), ('Country', 'Categorical', 'Country of origin. Categories are represented in the ISO 3155– 3:2013 format'), ('MarketSegment', 'Categorical', 'Market segment designation. In categories, the term “TA” means “Travel Agents” and “TO” means “Tour Operators”'), ('IsRepeatedGuest', 'Categorical', 'Value indicating if the booking name was from a repeated guest (1) or not (0)'), ('PreviousCancellations', 'Integer', 'Number of previous bookings that were cancelled by the customer prior to the current booking'), ('PreviousBookingsNotCanceled', 'Integer', 'Number of previous bookings not cancelled by the customer prior to the current booking'), ('ReservedRoomType', 'Categorical', 'Code of room type reserved. Code is presented instead of designation for anonymity reasons'), ('AssignedRoomType', 'Categorical', 'Code for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons'), ('BookingChanges', 'Integer', 'Number of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation'), ('DepositType', 'Categorical', 'Indication on if the customer made a deposit to guarantee the booking. This variable can assume three categories: No Deposit – no deposit was made. Non Refund – a deposit was made in the value of the total stay cost. Refundable – a deposit was made with a value under the total cost of stay.'), ('CustomerType', 'Categorical', 'Type of booking, assuming one of four categories: Contract - when the booking has an allotment or other type of contract associated to it; Group – when the booking is associated to a group; Transient – when the booking is not part of a group or contract, and is not associated to other transient booking; Transient-party – when the booking is transient, but is associated to at least other transient booking'), ('RequiredCardParkingSpaces', 'Integer', 'Number of car parking spaces required by the customer'), ('TotalOfSpecialRequests', 'Integer', 'Number of special requests made by the customer (e.g. twin bed or high floor)')],
            columns=['Variable', 'Type', 'Description'])
        
        with st.expander("Show variables"):
            st.table(hotel_original_features)
            
        '''The box plots below show the distribution for the numerical variables except for IsCancelled since that is the predictive variable. From the plots below, a sense of the distribution of the numerical data can start to be formed. Most of the data like Adults, Babies, Booking Changes, Children, Is Repeated Guest, Previous Bookings Not Canceled, Previous Cancellations, and Required Car Parking Spaces have many values that are zero or close to zero. Other variables like Lead Time, Stays In Weekend Nights, Stays In Week Nights and Total Of Special Requests have more variability than the variables mentioned above. These boxplots provide insight into the average number of observations for a given variable but also a sense of the outliers which are represented by black points. For example, while the average amount of adults was around 1, some bookings had upwards of 40+ adults.'''
        
        with st.expander("Show boxplots"):
            st.image('images/numvar.png')
        
        '''The next step involved looking for any NA values. Using the diagnose() function from the dlookr package, the table below was created. The first thing to note here is that there is no missing data. Additionally, the output shows the number of unique observations for each variable. Variables like lead time and country have more unique values than others indicating these could be distinguishing characteristics in later models.'''
        
        missing_data = pd.DataFrame(
            [('IsCanceled', 'numeric', '0', '0', '2', '0.00004992511'), ('LeadTime', 'numeric',	'0', '0','412',	'0.01028457314'), ('StaysInWeekendNights', 'numeric', '0','0','16','0.00039940090'),('StaysInWeekNights', 'numeric', '0','0','31','0.00077383924'),('Adults', 'numeric', '0','0','14','0.00034947579'),('Children', 'numeric', '0','0','5','0.00012481278'),('Babies', 'numeric', '0','0','3','0.00007488767'),('Meal', 'character', '0','0','5','0.00012481278'),('Country', 'character', '0','0','126','0.00314528208'),('MarketSegment', 'character', '0','0','6','0.00014977534'),('IsRepeatedGuest', 'numeric', '0','0','2','0.00004992511'),('PreviousCancellations', 'numeric', '0','0','11','0.00027458812'),('PreviousBookingsNotCanceled', 'numeric', '0','0','31','0.00077383924'),('ReservedRoomType', 'character', '0','0','10','0.00024962556'),('AssignedRoomType', 'character', '0','0','11','0.00027458812'),('BookingChanges', 'numeric', '0','0','15','0.00037443834'),('DepositType', 'character', '0','0','3','0.00007488767'),('CustomerType', 'character', '0','0','4','0.00009985022'),('RequiredCarParkingSpaces', 'numeric', '0','0','5','0.00012481278') ,('TotalOfSpecialRequests', 'numeric', '0','0','6','0.00014977534')],
            columns=['Variable', 'Type', 'Missing Count', 'Missing Percent', 'Unique Count', 'Unique Rate'])
        
        with st.expander("Show missing & unique counts"):
            st.table(missing_data)
            
        '''Since lead time had the most unique observations, a box plot was created to investigate this variable further, which is shown below. The x-axis represents lead time, in the number of days, and the y-axis represents if that booking was canceled (1) or not (0). For those who canceled their reservation, the lead time, on average, was greater than for those who did not cancel their reservation. To be more specific, the average lead time for those who did not cancel their reservation was around 79 days. The average lead time for those who did cancel their reservation was around 129 days.'''
        
        with st.expander("Show lead time boxplot"):
            st.image('images/leadtime.png')
            
        '''The other variable worth investigating at this point is the country column. In the figure below, each country that was in the dataset is colored according to the overall percent cancellation rate for that particular country where the bookings came from. The countries that were not in the dataset are colored grey. According to the plot, most of the country's cancellation rate were less than 50%. There are a few countries colored red indicating a 100% cancellation rate. This is a result of a few observations for that particular country. For example, the country colored red in Western Africa is Senegal where the only observation resulted in a canceled reservation. The same thought process applies to those countries' color white, indicating a 0% cancellation rate. There are 54 countries colored white with the most observations (23) coming from individuals located in Ukraine.'''
        
        with st.expander("Show percent cancellations by country"):
            st.image('images/percentcancel.png')
        
        '''To get a better idea of the distribution of countries in the dataset, a world map based on the total amount of observations per country was created which can be seen in the plot below. A few observations can be seen from this map. First, it is clear that there is one country that has over 17,000 observations present in the dataset. Upon further investigation, that country turned out to be Portugal. This means that individuals who say they are from Portugal represent around 44% of the total observations in the dataset. Additionally, the other frequently represented countries come from neighboring countries of Portugal (6814 from the UK, 3957 from Spain, and 2166 from Ireland). Without having prior knowledge about this dataset, it is safe to conclude that this data was collected from a hotel located somewhere in Portugal. This is supported by the fact that about 44% of bookings came from Portugal and around 70% came from neighboring countries.'''
        
        with st.expander("Show total observations per country"):
            st.image('images/observations.png')
            
        '''Since it is safe to assume this data was taken from a hotel located in Portugal, some feature engineering was conducted to create the box plot below. All the observations had a country associated with it where the center of that country could roughly be found using latitude and longitude coordinates. These coordinates could then be used to find the straight-line distance, in km, to the center of Portugal. In the plot below, the x-axis represents distance, in km, and the y-axis represents if that booking was canceled (1) or not (0). For those who did cancel their reservation, their approximate location was closer to Portugal, around 735km. On the other hand, for those who did not cancel their reservation, their approximate location was further from Portugal, around 1480km.'''
        
        with st.expander("Show distance from Portugal boxplot"):
            st.image('images/distancefromportugal.png')
            
        '''#### Association Rules'''
        
        '''To begin with the analysis of the data, association rules are often a good place to start. Association rules are a data mining technique used to discover interesting relationships between variables. These rules identify frequently occurring patterns and associations among variables which can help inform decision-making. Utilizing the arules package in R, it is possible to find different combinations (RHS) that lead to a certain outcome (LHS) which in this case is whether or not someone canceled their reservation. Some important parameters of association rules are support and confidence. Support represents the number of times that the RHS and LHS show up together. Confidence is the number of times the RHS shows up when the LHS is present. Both numbers are represented as percentages. Below is a plot that shows the different combinations of support and confidence colored by the corresponding lift value, which indicated interestingness. 2015 rules were found with minimum support of 0.08 and minimum confidence of 0.50.'''
        
        with st.expander("Show association rules plot"):
            st.image('images/2015rules.png')
    
        '''The table below shows five rules that are located on the top left side of the plot above. These five rules can provide insight into the different combinations of variables that lead someone to cancel their reservation. For example, when looking at the first rule, a manager could easily see that if there are two adults that are coming from Portugal, have never canceled before, are assigned room type A, and have no booking changes, in the past, this combination has resulted in a 72% cancellation rate. Although no prediction is being made, a hotel manager could use this information, along with the domain experience they have, to help inform their future decisions.'''
    
        assoc_rules = pd.DataFrame(
            [('Adults=2, Country=PRT, PreviousBookingsNotCanceled=0, AssignedRoomType=A, BookingChanges=0', '=> {IsCanceled=1}', '0.08689466', '0.7235502', '2.606134'), ('Adults=2, Country=PRT, IsRepeatedGuest=0,AssignedRoomType=A,BookingChanges=0,RequiredCarParkingSpaces=0', '=> {IsCanceled=1}','0.08582127','0.7242469', '2.608643'), ('Adults=2, Country=PRT, PreviousBookingsNotCanceled=0, ReservedRoomType=A, AssignedRoomType=A, BookingChanges=0, RequiredCarParkingSpaces=0', '=> {IsCanceled=1}', '0.08686970', '0.7242456', '2.608639'), ('Adults=2, Country=PRT, IsRepeatedGuest=0, ReservedRoomType=A, AssignedRoomType=A, BookingChanges=0, RequiredCarParkingSpaces=0', '=> {IsCanceled=1}', '0.08579631', '0.7247997', '2.610634'), ('Adults=2, Children=0, Country=PRT, PreviousBookingsNotCanceled=0, AssignedRoomType=A, BookingChanges=0, RequiredCarParkingSpaces=0', '=> {IsCanceled=1}', '0.08494758', '0.7266709', '2.617374')],
            columns=['LHS', 'RHS', 'Support', 'Confidence', 'Lift'])
        
        with st.expander("Show association rules table"):
            st.table(assoc_rules)
        
        '''#### SVM and Decision Tree Models'''
        
        col1_4, col2_4 = st.columns(2)
        
        with col1_4:
            '''The first model that was run was an SVM without k-fold cross-validation. SVM algorithms project data into a higher dimension and try to optimize the boundary between different cases. In this scenario, whether someone cancels their reservation or not. For testing purposes, 30% of the data was held out. The results of the model on the unseen data are displayed in the table to the right. This model accurately determined if someone canceled their reservation or not around 85% of the time (marked by accuracy). To be more precise, for those that we know did not cancel their reservations, the model predicted those cases right around 95% of the time (marked by sensitivity). On the other hand, for those that we know did cancel their reservation, the model predicted those cases right around 60% of the time (marked by specificity).'''
            
            '''Next, an SVM model was run with k-fold cross-validation. In this approach, SVM models are run ten different times. Each time the SVM runs, it holds out a certain amount of data to calculate the accuracy for each iteration. During this process, model tuning occurs, and the best overall model is found. The results of this model do not show much improvement in accuracy (86%) and a decline in sensitivity (94%). There is an improvement in specificity though, going from 60% to 66%. This increase in accurately predicting if someone will cancel outweighs the very minor drop in accurately predicting if someone does not cancel their reservation.'''
            
            '''SVM models do not provide helpful insight into their internal workings. If that is of importance to a hotel manager, a decision tree model would be beneficial. Since major improvements were found when using k-fold cross-validation, that same approach was used. A decision tree model is self-explanatory. The model will find the best place to split a node to help define if someone has canceled their reservation or not. When comparing the results of this model to the SVM model with k-fold cross-validation, the same trend appears. There is a slight improvement in accuracy (87%) and a minor drop in sensitivity (93%). But again, a major improvement in specificity is found, 66% to 72%. The decision tree model not only performed better but also provided insight into some of the most important variables. As theorized, lead time was the most important variable in predicting a cancellation.  Some of the other important variables were RequiredCarParkingSpaces (77%), CountryPRT (Portugal, 40%), and MarketSegmentOnline TA (34%).'''
            
            '''The final model that was run was once again a decision tree, but with fewer variables. The variables that were chosen to go into this model were based on the most important ones found in the previously mentioned decision tree model that used all the variables. This model performed almost identically to the decision tree model using all the variables. This is important because using fewer variables to achieve the same level of performance as a model that uses more variables is preferred. This reduces the complexity of the model and the burden of data collection. By using fewer variables, the workload for those responsible for predicting hotel cancellations is reduced.'''
            
        model_performance = pd.DataFrame(
            [('SVM without k-fold', '0.8480486', '0.9449372', '0.5959233'), ('SVM with k-fold', '0.8636931', '0.9402143', '0.6645683'), ('Decision tree with k-fold', '0.869851', '0.9281189', '0.7182254'), ('Decision tree with k-fold & less variables', '0.8681035', '0.9283493', '0.7113309')],
            columns=['Model', 'Accuracy', 'Sensitivity', 'Specificity'])
        
        dt_importance = pd.DataFrame(
            [('LeadTime', '100.000'),('RequiredCarParkingSpaces', '74.365'),('CountryPRT', '38.185'),('MarketSegmentOnline TA', '32.918'),('DepositTypeNon Ref', '27.701'),('PreviousCancellations', '26.404'),('StaysInWeekNights', '21.068'),('StaysInWeekendNights', '17.458'),('PreviousBookingsNotCanceled', '15.477'),('CustomerTypeTransient', '14.981'),('BookingChanges', '13.857'),('MarketSegmentDirect', '12.524'),('TotalOfSpecialRequests', '11.768'),('CustomerTypeTransient-Party', '9.480'),('MarketSegmentOffline TA/TO', '6.110'),('IsRepeatedGuest', '2.957'),('MarketSegmentGroups', '2.905'),('CountryDEU', '1.870'),('CountryESP', '1.731'),('CountryGBR', '1.126')],
            columns=['Feature', 'Importance'])
        
        with col2_4:
            '''##### Model performance'''
            st.table(model_performance)
            ''' ##### Most important features of the decision tree with k-fold and less variables'''
            st.table(dt_importance)
            
        '''This project focused on providing hotel managers with an in-depth analysis of the type of people who cancel hotel reservations and actionable insight into predicting who would cancel their reservation. Since this data most likely came from Portugal, it is worth noting that the analysis and models above would be more of a benefit to hotel managers located in or near Portugal. From the analysis done in this project, a manager has three options to inform their decisions. One would be to use past information to inform future decisions. In this case, the data exploration and association rules would be the most useful in this case. Another option would be to utilize the predictive models to predict if someone, based on the information the manager knows, will cancel their reservation. And of course, a manager could use a combination of both. Regardless of the path chosen, utilizing any of the data-driven results discussed above could have a profound impact on the profitability of a given hotel.'''
           
if choose == "Recorded Video Presentation":
    '''## Recorded Video Presentation'''
    '''I have not created this yet. I will before the final version!'''
    
if choose == "Blog Post":
    '''## Blog Post'''
    '''The Applied Data Science Master’s program at Syracuse University is designed to provide students with a strong foundation in data science, including skills in data analysis, machine learning, and statistical modeling. The program aims to prepare students for careers in a range of industries where data analysis and modeling skills are in demand. Students are also exposed to various tools and programming languages commonly used in the industry, such as Python, R, and SQL.'''
    
    '''To be honest, before starting the program, I did not know what to expect. At the time, I was out of school for two years with my senior year in undergrad being cut short by COVID. So, when I started the program, it was a little bit nerve-racking. During my job before starting graduate school, I was mainly programming in Python and VBA. One of the misconceptions I had about the program coming in was that all the classes would be using R, which added to my nerves since I have never worked with the language. Those nerves quickly subdued when taking IST 687, Introduction to Data Science. In this class, I began to get comfortable with R and got the hang of it early on. Instead of jumping right into building models, we learned how to use R starting with the basics and proceeded to build our way up. The learning in lectures was reinforced with hands-on labs which helped accelerate the learning process. On top of that, at the end of the semester, I was able to complete and end to end data science project, all in R. This project, as well as the project I completed in IST 707, Applied Machine Learning, were both effective in solidifying my understanding of the programming language R and both projects ended with the generation of actionable insight.'''
    
    '''Python, to my pleasant surprise, was utilized in most of the classes I took. As I previously mentioned, I already had a core understanding of Python as I worked with it in industry and had experience writing scripts. In the Applied Data Science program, I was able to further deepen my understanding of the language, particularly how it is used in machine learning. One class that focused solely on Python was IST 652, Scripting for Data Analysis. This class was straightforward as it merely taught the basics of Python, something I already had a good grasp on. Nonetheless, it was good reinforcing learning. The bulk of new knowledge that I learned was taught in IST 707 and IST 718, Big Data Analytics. IST 707 taught me the theory behind industry standard machine learning techniques and algorithms as well as how to build and tune these models all in Python. This learning was reinforced in IST 718 but with an emphasis on big data. In this class, I learned how to use Apache Hadoop to manage big data sets and Apache Spark to process those datasets. IST 652, 707, and 718 concluded in an end-to-end data science project with 652 ending in an in-depth analysis and 707 and 718 ending in actionable insights. All three classes helped improve my knowledge and skills in Python.'''
    
    '''The last two paragraphs served as a means of showing my achievement of learning objective four. I will now discuss how I achieved learning objective one which focused on using applicable technologies to collect, store, and access data. Coming into the program, I had no experience working with SQL and thought that Excel was the only way to store data. I quickly learned in IST 659, Data Administration Concepts & Database Management, but that was not the case. This course harped on the importance of using relational databases and how to properly configure and set them up. We also learned how powerful SQL was in getting data out and in the format we wanted for the inquiries trying to be answered. At the end of the semester, two peers and I were tasked with building a relational database from scratch. We needed to find our own data, design the database structure, and create a working implementation that users could use. This experience greatly increased my ability to work with relational databases. My knowledge of databases further expanded from taking IST 769, Advanced Database Management. I learned that there is data that simply cannot be stored in relational databases. Semi-structured data such as JSON or XML documents and unstructured data such as text, images, and video, necessitate other database designs to ensure proper storage. I learned how to configure and query multiple databases like document, wide column, key-value, graph, and streaming databases. My learning was reinforced through hands-on labs that focused on a different database each week. At the end of the semester, I completed a project where I had to gather my own data and put it into a database of my choosing. I built a graph database that contained multiple electronic products with connections being made to customers, companies, and categories. Both IST 659 and 769 expanded my knowledge of databases, something that I did not have coming into the program.  Through both lectures, class work, and end-of-the-semester projects, I demonstrated I acquired the ability to collect, store, and access data for the task at hand.'''
    
    '''I think that learning objectives two and three can be combined into the following sentence: apply visualizations, predictive models, and the full data science life cycle to produce actionable insight across a range of contexts. I completed several end-of-semester projects that required me to provide actionable insights to different stakeholders. For instance, in IST 687, I provided insights to hotel managers on guest cancellations, while in IST 718, my group and I provided insights to STEM students on future salaries based on their qualifications. Similarly, in IST 707, two peers and I provided insights to NFL coaches on the predictability of other teams. For each project, I followed the data science life cycle, which included problem definition, data collection and preparation, data exploration and analysis, and model building and validation. Additionally, I created various visualizations to help myself and my audience better understand the data. By completing these projects, I believe I effectively demonstrated my ability to create actionable insights, build models, create visualizations, and apply the full data science life cycle.'''
    
    '''The fifth learning objective of the Applied Data Science Master’s program at Syracuse University is focused on teaching students how to effectively communicate insights to a broad range of audiences. For me, and many others, public speaking can be a daunting task. Fortunately, the program provided numerous opportunities for me to improve my public speaking skills. In almost every class I took, there was some sort of presentation that I had to do. For example, in IST 707, I had to give three different presentations to the class about our group project. Similarly, in IST 719 (Information Visualization), I had to do an advanced topic presentation as well as a poster presentation at the end of the semester. For each presentation, I had to tailor my material to the audience. In our first presentation in IST 707, no one knew what we were talking about, so we had to keep it at a high level. In later presentations, we could dive further into topics as our audience had a general idea of what we were discussing. Similarly, in both presentations in IST 719, I had to keep everything at a high level since it was new material for the audience. I completed numerous presentations in other classes, which helped me get better at public speaking - something that was a weakness of mine. Over the course of the program, I gradually became more comfortable and better at it. I am now able to confidently speak in front of a crowd and deliver the insights, visualizations, and analytics that I need in an appropriate manner.'''
    
    '''The last learning objective focuses on applying ethics as it applies to using data and predictive models. This is a very important topic to cover as a data scientist as the consequences of not doing so could have a profound impact. The purpose of talking about ethics is to ensure that potential biases are identified and addressed. In IST 687, a class that all students must take their first semester, we discussed ethical challenges and how to frame certain questions. In IST 719, which was focused on information visualization, we discussed ethics in the context of creating effective and ethical visualizations. We learned about the dangers of misleading visualizations and how to avoid them by accurately representing the data and avoiding intentional or unintentional misinterpretation. I applied my learning in these classes to others when dealing with data that was going into predictive models and I think I did an adequate job doing so. While I feel that the program could have placed more emphasis on ethics in some classes, I still feel well-equipped to handle ethical challenges that may arise in my future work as a data scientist. I know how to identify potential biases and how to avoid them, and I have a deep understanding of the ethical implications of the work that I do.'''
    
    '''I was able to effectively apply the above-mentioned learning objectives through projects that I completed at the end of the semester. I independently undertook a project in IST 687 that involved analyzing hotel stay data to identify significant factors contributing to hotel reservation cancellations and developing predictive models for identifying potential cancellations. I applied the data science pipeline and skills learned in class to analyze the data and derive meaningful insights, including preliminary data exploration using box plots and histograms to identify general trends in the data, creating association rules to help identify patterns and relationships in the data, and utilizing SVM and decision tree models to make predictions. The decision tree model that used all 20 variables had the highest accuracy (87%) and specificity (72%), highlighting lead time, required parking spaces, and the booking party's country of residence as the most significant variables. This project significantly contributed to my education as an applied data science major. By undertaking the project, I applied the data science pipeline and skills learned during the IST 687 course to analyze real-life hotel stay data and derive meaningful insights. Through this project, I gained valuable experience in using R to clean and model data, and I developed the ability to create compelling visualizations to convey information. Additionally, the culminating presentation of my work required me to demonstrate my technical knowledge to peers, providing me with valuable presentation skills that I can apply to future collaborations.'''
    
    '''Another project I undertook was in IST 652 where I worked with three other classmates on a project focused on Python scripting for data analysis. We chose to use the World Resources Institute's dataset on power plants across different countries, with data spanning from 2013 to 2019. Our goal was to utilize Python to conduct in-depth analysis and answer our own research questions, rather than running models. Our analysis aimed to identify which countries produced the most renewable/non-renewable energy and which energy types were most prevalent based on the number of plants and fuel capacity. We also compared energy production and consumption across different countries. Overall, our analysis showed that despite the emphasis on renewable energy, most of the world's energy still comes from non-renewable sources. My specific task was to locate a population dataset and examine gigawatt-hour production per capita. Through our analysis, we found that Norway and Israel were leading countries in energy exports in 2017, and produced more than 0.015 gigawatt-hours per person, exceeding the average consumer usage per year of 0.01 gigawatt-hour. This project helped me improve my Python scripting skills for data analysis, particularly with operations such as group-by and table pivoting. It also taught me how to manipulate and clean large datasets and draw conclusions using Python packages like Plotly and Seaborn. Overall, it was a valuable real-world experience that I can apply in industry as a data scientist.'''
    
    '''Additionally, I completed a group project in Applied Machine Learning, IST 707, where my classmates and I aimed to apply machine learning techniques and algorithms to solve a real-world business problem. Our group chose to analyze the game of American football through the National Football League (NFL), specifically aiming to predict whether a given play would be a run or a pass. We obtained play-by-play data from the nflfastR library in R for the 2011-2020 seasons and used Python to build our predictive models. Through our analysis, we provided actionable insights for NFL teams and coaches. Our gradient boosting model accurately predicted whether a play would be a run or a pass 74.4% of the time for any team playing in the 2020 season. Additionally, we created 32 different team-based models, allowing us to break down a team's predictability by quarter and down. Our analysis revealed that the most predictable team in the 2020 season was the Jacksonville Jaguars (80%), while the least predictable was the Chicago Bears (74%). Throughout the project, I did most of the data cleaning and feature engineering as I was the domain expert in our group. This involved tasks such as filling in missing values, manipulating variables to indicate the productivity of the previous play, and computing the number of runs and passes. Additionally, I developed and fine-tuned the decision trees and gradient-boosting algorithms used in our analysis. Overall, this project allowed me to effectively apply all six of the program's learning outcomes. It required me to collect and store data in an efficient manner, use a mix of R and Python for data preprocessing and model building, communicate findings to a non-expert audience through presentations and visualizations, and ensure that our data was unbiased and accurately represented the true state of the game. I learned not only technical skills but also the importance of teamwork and collaboration in achieving a common goal.'''
    
    '''When it comes my favorite class, IST 659, Data Administration Concepts and Database Management stands out in the program for several reasons. Firstly, the course provided a comprehensive overview of the fundamental concepts and principles of database management systems (DBMS). I gained a deep understanding of data structures, file organizations, and various data models, including hierarchical, network, and relational models. Additionally, I learned entity-relationship modeling, data normalization, and database design techniques, which are crucial for developing efficient and scalable databases. One of the highlights of the course was the hands-on experience in database design and implementation. Through assignments, lab exercises, and course projects, I was able to apply the theoretical concepts learned in class to real-world scenarios. This practical approach allowed me to develop a solid foundation in designing and managing databases and honed my skills in using Structured Query Language (SQL) for data manipulation and query optimization. Throughout the course, I often found myself excited to work with SQL to query databases. I find SQL to be a fun, yet challenging language at the same time which fuels me to learn it more. Additionally, the course introduced advanced topics such as transaction management, concurrency control, distributed databases, and query performance optimization, which added depth to my understanding of modern database concepts and applications. This expanded my knowledge beyond basic database management and prepared me to tackle complex real-world data management challenges.'''
    
    '''One of the most rewarding aspects of the program, in my opinion, was the opportunity to work on end-of-the-semester projects in some of the classes. These projects allowed me to apply the knowledge and skills I acquired throughout the semester to real-world data science problems. Whether working on a project independently or with fellow peers, these end-of-the-semester projects served as a culmination of my learning and provided a platform to showcase my abilities. The projects were designed to be comprehensive, requiring me to go through the entire data science workflow from start to finish. This included tasks such as data collection, data cleaning, exploratory data analysis, feature engineering, model selection, model evaluation, and interpretation of results, all things a data scientist needs to know how to do. This hands-on experience provided invaluable opportunities to practice and reinforce the concepts and techniques I learned in class. Working on the end-of-the-semester projects also honed my ability to work effectively in a team, as some projects were collaborative in nature. Collaborating with fellow peers allowed for diverse perspectives, creativity, and innovation in problem-solving. It also provided opportunities to learn from my peers' strengths and expertise, further enhancing my skills and knowledge. Through these end-of-the-semester projects, I gained confidence in my ability to tackle real-world data science projects from start to finish and developed a deep understanding of the practical aspects of data science. These projects not only reinforced my learning but also equipped me with the skills and experience necessary to tackle real-world data science challenges beyond the classroom.'''
    
    '''In conclusion, my time as an Applied Data Science Master’s student at Syracuse University has been a transformative journey that has equipped me with the skills and knowledge necessary to excel in the field of data science. Through a comprehensive curriculum that covered the full data science life cycle and emphasized practical applications, I have been able to achieve the six learning objectives of the program. I have learned how to effectively collect, store, and access data using various technologies, and apply visualization and predictive models to generate actionable insights in diverse contexts, from predicting NFL outcomes to analyzing worldwide power plant data and hotel cancellations. I have gained proficiency in programming languages such as R and Python, which have become invaluable tools in my data science toolkit. Furthermore, the program has honed my communication skills, enabling me to effectively convey complex findings and insights to diverse audiences. I have also developed a strong ethical foundation, applying principles of fairness, transparency, and privacy in the development, use, and evaluation of data and predictive models. Overall, my experience at Syracuse University as an Applied Data Science Master’s student has been enriching, empowering, and has prepared me to thrive in the dynamic and ever-evolving field of data science. I am grateful for the opportunities, knowledge, and skills gained, and I am excited to apply them in my future endeavors as a data scientist.'''
    
if choose == "Contact":
    '''## Contact'''
    
    '''#### School Email'''
    '''kamcgee@syr.edu'''
    
    '''#### Personal Email'''
    '''kyle.mcgee34@gmail.com'''
    
    '''#### LinkedIn'''
    '''[Click Here](https://www.linkedin.com/in/kyle-mcgee-/)'''